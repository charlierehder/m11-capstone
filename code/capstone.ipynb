{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c2d9d7",
   "metadata": {},
   "source": [
    "## Team Rocket Payment Prediction Model Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bee2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymssql\n",
    "import yaml\n",
    "from yaml import Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44814bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open yaml to get database connection strings\n",
    "with open('secrets.yaml', 'r') as f:\n",
    "    configs = yaml.load(f, Loader=Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b790d65",
   "metadata": {},
   "source": [
    "#### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7cf2dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define database connection vars for future quer\n",
    "server = configs['data']['server']\n",
    "user = configs['data']['user']\n",
    "password = configs['data']['password']\n",
    "database = configs['data']['database']\n",
    "\n",
    "# define table strings\n",
    "efficiency_table = 'dbo.EfficiencyScores'\n",
    "safety_table = 'dbo.SafetyScores'\n",
    "outcomes_table = 'dbo.ClinicalOutcomeScores'\n",
    "community_table = 'dbo.EngagementScores'\n",
    "payment_table = 'dbo.PaymentAndValueOfCareVals'\n",
    "state_table = 'dbo.States'\n",
    "location_table = 'dbo.Locations'\n",
    "\n",
    "try:\n",
    "    # connect to database with above credentials\n",
    "    conn = pymssql.connect(server, user, password, database)\n",
    "    \n",
    "    # instantiate cursor\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # get efficiency data\n",
    "    efficiency_query = f'SELECT * FROM {efficiency_table}'\n",
    "    efficiency = pd.read_sql(efficiency_query, conn, index_col='Efficiency_ID')\n",
    "    \n",
    "    # get safety data\n",
    "    safety_query = f'SELECT * FROM {safety_table}'\n",
    "    safety = pd.read_sql(safety_query, conn, index_col='Safety_ID')\n",
    "    \n",
    "    # get outcomes data\n",
    "    outcomes_query = f'SELECT * FROM {outcomes_table}'\n",
    "    outcomes = pd.read_sql(outcomes_query, conn, index_col='ClinicalOutcome_ID')\n",
    "    \n",
    "    # get community data\n",
    "    community_query = f'SELECT * FROM {community_table}'\n",
    "    community = pd.read_sql(community_query, conn, 'EngagementScore_ID')\n",
    "    \n",
    "    #get payment data\n",
    "    payment_query = f'SELECT * FROM {payment_table}'\n",
    "    payment = pd.read_sql(payment_query, conn, index_col='Payment_ID')\n",
    "    \n",
    "    #get state data\n",
    "    state_query = f'SELECT * FROM {state_table}'\n",
    "    state = pd.read_sql(state_query, conn, index_col='State_ID')\n",
    "    \n",
    "    #get location data\n",
    "    location_query = f'SELECT * FROM {location_table}'\n",
    "    location = pd.read_sql(location_query, conn, index_col='Facility_ID')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c38eec",
   "metadata": {},
   "source": [
    "The efficiency, outcomes, community and safety table are all the same size and contain the facility ID's that we have the corresponding data for. Notice the locations and payment table are much larger and contain facilities that other tables don't have data on. These rows will be dropped in the following merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d70f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joined = efficiency.merge(safety, on='Facility_ID', how='inner')\n",
    "joined_1  = joined.merge(outcomes, on='Facility_ID', how='inner')\n",
    "joined_2 = joined_1.merge(community, on='Facility_ID', how='inner')\n",
    "joined_3 = joined_2.merge(location, on='Facility_ID', how='left')\n",
    "joined_4 = joined_3.merge(state, on='State_ID', how='left')\n",
    "final_join = payment.merge(joined_4, on='Facility_ID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4291f",
   "metadata": {},
   "source": [
    "We'll drop the folowing columns as they contain non-numeric, non-categorical data that would be no use to any of the models used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c95723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop uneeded columns\n",
    "dropped = final_join.drop(['Facility_ID', 'Facility_Name', 'Location_City', 'Location_State', 'Location_Zip_Code', 'Location_County', 'State_Name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f59b1",
   "metadata": {},
   "source": [
    "The following variables were already label encoded but not with values that made sense in that I think the labels were standardized which isn't necessary. Below we redo this label encoding and followed by a one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6fbc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode categorical columns appropriatelyl\n",
    "dropped['Payment_Category'] = dropped['Payment_Category'].astype('category').cat.codes\n",
    "dropped['Value_Of_Care_Category'] = dropped['Value_Of_Care_Category'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20536dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the label encoded columns\n",
    "pc_onehot = pd.get_dummies(dropped['Payment_Category'])\n",
    "dropped = dropped.drop('Payment_Category', axis=1)\n",
    "dropped = dropped.join(pc_onehot, lsuffix='_Pay')\n",
    "\n",
    "vc_onehot = pd.get_dummies(dropped['Value_Of_Care_Category'])\n",
    "dropped = dropped.drop('Value_Of_Care_Category', axis=1)\n",
    "dropped = dropped.join(vc_onehot, rsuffix='_VC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502b3f8",
   "metadata": {},
   "source": [
    "#### Payment Estimates Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d55c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7e966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998155914442716\n"
     ]
    }
   ],
   "source": [
    "X = dropped[['Lower_Estimate', 'Higher_Estimate']]\n",
    "y = dropped['Payment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lnrg = LinearRegression()\n",
    "lnrg = lnrg.fit(X_train, y_train)\n",
    "y_preds = lnrg.predict(X_test)\n",
    "print(r2_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d121bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving/exporting model\n",
    "estimates_model = joblib.dump(lnrg, 'estimates_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88f2fc",
   "metadata": {},
   "source": [
    "#### Linear Regression Payment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f85bd90",
   "metadata": {},
   "source": [
    "Started with a linear regression model to get a baseline of what to expect for regressions on this data. No cross validation was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0dca63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dropped.drop(['Payment', 'Higher_Estimate', 'Lower_Estimate'], axis=1)\n",
    "y = dropped['Payment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be034add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6434151116740889\n"
     ]
    }
   ],
   "source": [
    "lnrg = LinearRegression()\n",
    "lnrg = lnrg.fit(X_train, y_train)\n",
    "y_preds = lnrg.predict(X_test)\n",
    "print(r2_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c12b5",
   "metadata": {},
   "source": [
    "#### Lasso Regression Payment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc666c3d",
   "metadata": {},
   "source": [
    "After a pretty middling performance from our linear regression model we decided to exhaust the linear regression idea after trying just one more variant. The variant chosen was Lasso Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3124d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105908017.92247444, tolerance: 356501.32587910385\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42888634.16191161, tolerance: 358057.7856691777\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 43247054.987890124, tolerance: 363547.952536348\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 153755782.7852139, tolerance: 357043.7875319446\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 45635334.00273383, tolerance: 351877.83129207103\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105938362.92926311, tolerance: 356501.32587910385\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42907918.31826413, tolerance: 358057.7856691777\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 43265396.32004559, tolerance: 363547.952536348\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 153791758.63899416, tolerance: 357043.7875319446\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 45655454.923704505, tolerance: 351877.83129207103\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10818341.08806181, tolerance: 356501.32587910385\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22449615.45655656, tolerance: 358057.7856691777\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 43444778.416716814, tolerance: 363547.952536348\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44874156.781690836, tolerance: 357043.7875319446\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11597368.610375047, tolerance: 351877.83129207103\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1845266.4485553503, tolerance: 356501.32587910385\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1819660.8875738382, tolerance: 358057.7856691777\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1743392.459732771, tolerance: 363547.952536348\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1883633.9333865643, tolerance: 357043.7875319446\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/charlie/m11-capstone/env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1968098.2724820375, tolerance: 351877.83129207103\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6420533626830127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "la_params = {'alpha': [0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "larg = Lasso()\n",
    "la_grid = GridSearchCV(larg, la_params, scoring='r2', cv=5)\n",
    "la_grid.fit(X_train, y_train)\n",
    "y_preds = la_grid.predict(X_test)\n",
    "print(r2_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fc63a",
   "metadata": {},
   "source": [
    "The performance from this model was almost identical to that of the original linear regression at least in termns of its r-squared value which we are using in this case to be a general performance metric and quick comparison point between models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce53659",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c92fdf",
   "metadata": {},
   "source": [
    "Moving over to ensemble methods we decided to first try a random forest regressor to see if we get any improvments over a standard regression. We support this with a bit of hyperparameter tuning with respect to the number of estimators, the maximum number of features, the mimimum number of samples to warrant a split and whether or not bootstrap samples are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d05fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6760349109986256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_params = { \"n_estimators\"      : [10,20,30],\n",
    "              \"max_features\"      : ['sqrt', 'log2'],\n",
    "              \"min_samples_split\" : [2,4,8],\n",
    "              \"bootstrap\": [True, False],\n",
    "            }\n",
    "\n",
    "rfrg = RandomForestRegressor()\n",
    "rf_grid = GridSearchCV(rfrg, rf_params, scoring='r2', cv=5)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "y_preds = rf_grid.predict(X_test)\n",
    "print(r2_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec036b7",
   "metadata": {},
   "source": [
    "At first glance we see a noticeable improvement in the r-squared value of this model compared to the previous two. Ensemble methods seem to be getting more promising results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adee817",
   "metadata": {},
   "source": [
    "#### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de144d8",
   "metadata": {},
   "source": [
    "What many consider to be the be-all and end-all of statistical models, XGBoost is the logical next step in maximizing ensemble regressors. This is supported by cross validation on the learning rate of the model, the maximum depth of each tree in the model, the minimum child weight of a node, the gamma of the model and the number of columns used in each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "265f674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7830035159585188\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xg_params = {'nthread':[4],\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [.03, 0.05, .07],\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]\n",
    "            }\n",
    "\n",
    "'''\n",
    "xg_params = {'learning_rate': (0.05, 0.10, 0.15),\n",
    "             'max_depth': [3, 4, 5, 6, 8],\n",
    "             'min_child_weight': [1, 3, 5, 7],\n",
    "             'reg_alpha':[],\n",
    "             'reg_lambda':[],\n",
    "             'gamma':[0.0, 0.1, 0.2],\n",
    "             'colsample_bytree':[0.3, 0.4],}\n",
    "'''\n",
    "\n",
    "xgrg = XGBRegressor()\n",
    "xg_grid = GridSearchCV(xgrg, xg_params, scoring='r2', cv=2, n_jobs=5)\n",
    "xg_grid.fit(X_train, y_train)\n",
    "y_preds = xg_grid.predict(X_test)\n",
    "print(r2_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4378240",
   "metadata": {},
   "source": [
    "This R-squared value is a marked improvement over the Random Forest and Linear regressors from above. Our model accounts for about 80% of the variance of the dependent variable which implies there is correlation to be found in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a461ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585994.229443796\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc3a0a",
   "metadata": {},
   "source": [
    "If we take a look at the mean squared error of the model we find the model has an average residual of about about $765. With respect to the payouts of medicare/medicaid for different procedures this is an acceptable error. Below you'll see the parameters used in the best performing version of our XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c849cf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7,\n",
       " 'learning_rate': 0.03,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 4,\n",
       " 'n_estimators': 500,\n",
       " 'nthread': 4,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0bbf73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_model.model']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shippin' it\n",
    "joblib.dump(xg_grid, 'xgboost_model.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
